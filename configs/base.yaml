model_name: Qwen/Qwen2.5-0.5B
max_length: 256

seed: 42
device_map: auto

# training defaults (overridden by CLI during sweeps)
train:
  per_device_batch_size: 1
  grad_accum_steps: 8
  lr: 2.0e-4
  epochs: 2
  warmup_ratio: 0.03
  weight_decay: 0.0
  max_train_examples: null   # IMPORTANT: null means full by default

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules: ["q_proj", "v_proj"]
